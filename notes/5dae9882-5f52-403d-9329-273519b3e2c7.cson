createdAt: "2018-04-28T15:12:21.189Z"
updatedAt: "2018-04-28T17:17:40.328Z"
type: "MARKDOWN_NOTE"
folder: "fc410573c89e5aa6eb30"
title: "Structuring Machine Learning Projects"
content: '''
  # Structuring Machine Learning Projects
  
  # Week 2 - Error Analysis
  
  ## Carrying out error analysis [Video]
  - Analyize whether solving the problem for dogs will benefit you a lot or not.
  - Evaluate multiple ideas in parallel
  - Make a spreadsheet for different ideas and calculate/evaluate benefits from each one to decide what to work on
  
  ## Cleaning up incorrectly labeled data [Video]
  - Error on labels in training data
  - DL algos quite robust to random errors in training set (maybe leave them as it is)
  - DL algos less robust to systematic errors
  - Include these incorrect labels in error analysis
  - If impact of labels is big try fixing otherwise not worth
  - Apply same process to dev/test set.
  - Consider examples algo got right as well.
  - Train and dev/test might come from slightly different distributions
  
  ## Build your first system quickly, then iterate [Video]
  - Setup dev/test set and metric
  - Build initial version quickly
  - Use bias/variance & error analysis to prioritize next steps
  - first fast => iterate unless you are trying to invent a new thing (algo)
  
  # Week 2 - Mismatching training and dev/test set
  
  ## Training and testing on different distributions [Video]
  - data from web vs app users (blurry)
  - keep app in dev/test set. so that we optimize for it even though number of things are less.
  - Buying training data for different use case?
  
  ## Bias and Variance with mismatched data distributions [Video]
  - Training-dev set: same distribution as training set, but not used for training
  - Variance: training error <=> training-dev error
  - Data-mismatch: training-dev error <=> dev error
  - _[ERRORS]_ human level **< avoidable bias >** training **< variance >** training-dev **< data mismatch >**  dev **< degree of overfitting >** test
  
  ## Addressing data mismatch [Video]
  - Carry out manual error analysis to try to understand difference between training and dev/test sets
  - Make training data more or similar or collect more data similar to dev/test sets.
  - artificial data synthesis (add car noise to speechs)
  - overfitting to same noise or same artifical data?. Get more noise.
  
  # Week 2 - Learning from multiple tasks
  
  ## Transfer learning [Video]
  - image recognition to radiology diagnosis (change last layer ). Transfer learning
  - When makes sense => a lot of data problem to = less data problem
  - Transfer from A => B, A, B has same input x and you have more data on task A, low level features from A could be helpful for learning B
  
  ## Multi-task learning [Video]
  - One NN try to do multiple tasks from the start
  - Unlike softmax regression, a image can have multiple labels
  - Loss function consider all lalbels
  - When makes sense: 
  - Training on a set of tasks that could benefit from having shared lower-level features
  - Usually amount of data we have data for each task is similar.
  - Can traing a big NN to do well on all tasks.
  
  # Week 2 - End-to-end deep-learning
  
  ## What is end-to-end deep learning? [Video]
  - audio => features => phonemes => words => transcript
  - audio => transcript [end-to-end]
  - smaller data original one works fine, bigger data end-to-end learning works.
  - Multi step apporach: detect face => from face identity found.
  - You can find a lot of data for detecting face and a lot of data for identity task well.
  - e.g machine translation
  - image -> bones -> age vs image -> age
  
  ## Whether to use end-to-end deep learning [Video]
  - **Pros**:
    - Let data speak
    - Less hand designing components needed
  - **Cons**:
    - May need large amount of data
    - Excludes potentially useful hand-designed components
  - Key question do you have sufficient data to learn a function of the complexity needed to map x -> y
  - Use DL to learn individual components
'''
tags: []
isStarred: false
isTrashed: false
