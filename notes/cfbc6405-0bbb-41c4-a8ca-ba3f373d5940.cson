createdAt: "2018-04-01T16:11:54.099Z"
updatedAt: "2018-04-02T19:30:28.609Z"
type: "MARKDOWN_NOTE"
folder: "fc410573c89e5aa6eb30"
title: "Neural Networks and Deep Learning"
content: '''
  # Neural Networks and Deep Learning
  # Week 2
  
  ## Binary Classification [Video]
  - e.g: Image -> 1, 0 (cat or not)
  - Single column feature vector **x**
  - Image size 64x64 => nx/n = 64 * 64 * 3 = 12288
  - x -> y
  - (x, y) x is n dimenional feature vector, y 0 1
  - training examples (x1, y1) .. (xm, ym). m => # of training examples
  - m_train, m_test
  - X => columns as training examples, m columns, nx rows (# of features). X => [nx]**x**[m] matrix. X.shape=(nx, m)
  - Y => m columns of ys. Y => [1]**x**[m] matrix. Y.shape=(1, m)
  
  ## Logistic Regression [Video]
  - Used for binary classification problems
  - given x want y = P(y = 1 | x)
  - x, nx dimentional vector
  - Parameters for logistic regression: **w** nx dimentional vector and **b** real number
  - Output y^ = sigmoid(w^T * x + b).
  - w^T * x + b = z
  - sigmoid(z) = 1 / 1 + e^-z
  - z large => y converges to 1, z small => y converges 0
  
  ## Logistic Regression Cost Function [Video]
  - To train parameters w and b we need a cost funtion
  - want y^(i) ~= y(i)
  - Loss (error) function: L(y^, y) = -(y * logy^ + (1 - y) * log(1 - y^))
  - if y = 1 L(y^, y)  = -logy^, we want logy^: large => y^: large
  - if y = 0 L(y^, y) = -log(1 - y^), we want log(1 - y^): large => y^: small
  - Cost function J(w, b) = 1/m * SUM 1..m : L(y^(i), y(i)) = -1/m * SUM 1..m: [ y(i) * logy^(i) + (1 - y(i)) * log(1 - y^(i)) ]
  - Loss function applied to single training example
  - Cost function is cost of parameters, avg of loss function for all training items
  - Logistic Regression very very small neural network
  
  ## Gradient Descent [Video]
  - algorithm to learn w,b in logistic regression
  - We want to find w,b makes J(w, b) smallest
  - convex surface with 3 dimensions, w, b and J(w, b)
  - w := w - alpha * dJ(w, b) / dw => alpha learning rate * partial derivate
  - b := b - alpha * dJ(w, b) / db => alpha learning rate *  partial derivate 
  
  ## Derivatives [Video]
  - f(a) = 3 * a, f'(a) = 3, df(a)/da = 3, slope
  
  ## More Derivaite Examples [Video]
  - f(a) = a ^ 2, f'(a) = 2 * a
  - f(a) = a ^ z , f'(a) = z * a^(z - 1)
  - f(a) = ln(a), f'(a) = 1 / a
  
  ## Computation graph [Video]
  - J(a, b, c) = 3 * (a + b * c)
  - u = b * c
  - v = a + u
  - J = u * v
  
  ## Derivates with a Computation Graph [Video]
  - chain rule to compute derivates
  
  ## Logistic Regression Gradient Descent [Video]
  - z = w^T * x + b
  - y^ = a = sigmoid(z)
  - L(a, y) = - (y * log(a) + (1 - y) * log(1 - a))
  - derivation, backwards propogation, chain rule
  
  ## Gradient Descent on m Examples [Video]
  - coss function J(w, b) = 1/m * SUM 1..m L(a(i), y)
  - a(i) = y^(i) = sigmoid(z(i)) = sigmoid(w^T * x(i) + b)
  - dJ(w, b)/dw => compute for all w
  ```
  # single step of gradient descent
  # 2 parameters in w
  J, dw1, dw2, db = 0
  for i = 1 to m:
    z(i) = w^T * x(i) + b
    a(i) = sigmoid(z(i))
    J += -[ y(i) * loga(i) + (1 - y(i)) * log(1 - a(i)) ]
    dz(i) = a(i) - y(i)
    
    # for loop over all parameters of w actually
    dw1 += x1(i) * dz(i)
    dw2 += x2(i) * dz(i)
    # end for
    
    db += dz(i)
  J, dw1, dw2, db /= m
  
  w1 := w1 - alpha * dw1
  w2 := w2 - alpha * dw2
  b := b - alpha * db
  
  ```
  
  ## Vectorization [Video]
  - z = w^T * x + b where w and x are vectors
  - ``` z = np.dot(w, x) + b``` python numpy version
  - vectorization is like 300 times faster than for loop
  
  ## More Vectorization Examples [Video]
  - whenever possible avoid explicit for loops
  - u = Av
  - u_i = SUM Aij * vj
  ```
  # non-vectorized
  u = np.zeros(n, 1)
  for i ...
    for j ...
      A[i] += A[i][j] * v[j]
      
  # vectorized
  u = np,dot(A, v)
  ```
  
  ```
  import numpy as np
  u = np.exp(v)
  np.log(v)
  np.abs(v)
  np.maximum(v, 0)
  ```
  
  ## Vectorizing Logistic Regression [Video]
  - **X** (nx, m)
  - **Z** (1, m) = w^T * X + b
  - ``` Z = np.dot(w.T, x) + b```
  - b is a real number, implicitly expanded to a vector by python: **Broadcasting**
  - **A** = sigmoid(Z)
  
  ## Vectorizing Logistic Regression's Gradient Output [Video]
  - **dz(i)** = a(i) - y(i)
  - **dZ** (1, m)
  - **A** = [ a(1) .. a(m) ]
  - **Y** = [ y(1) .. y(m) ]
  - **dZ** = A - Y
  ```
  # single iteration of gradident descent for logistic regression
  # need for loop over this
  Z = np.dot(w.T, X) + b
  A = sigmoid(Z)
  dZ = A - Y
  dw = 1/m  * X * dZ^T
  db = 1/m * np.sum(dZ)
  w := w - alpha * dw
  b := b - alpha * db
  ```
  
  ## Broadcasting in Python [Video]
  ```
  cal = A.sum(axis=0)
  percentage(100 * A/cal)
  ```
  - **reshape** is O(1)
  - (m, n) +,-,*,/ (1, n) => (m, n)
  - (m, n) +,-,*,/ (m, 1) => (m, n)
  
  ## A note on python/numpy vectors [Video]
  - get alone done single line of code
  - implicit stuff => more prune to bugs e.g: broadcasting
  - no dimension mismatch erros can be dangerous
  - do not use rank1 arrays
  ```
  # don't use this rank1 array
  a = np.random.randn(5)
  # a.shape() => (5, )
  
  a = np.random.randn(5, 1)
  # a.shape() => 5, 1 , col vector
  
  a = np.random.randn(1, 5)
  # a.shape() => 1, 5, row vector
  
  # cheap op, easy to eliminate bugs, use freely
  assert(a.shape == (5, 1))
  
  # use freely if you are not sure
  a = a.reshape((5, 1))
  ```
  
  ## Quick tour of Jupyter/iPython Notebooks [Video]
  - ipython for assignments
  - remember to execute import blocks
  
  ## Explanation of logistic regression cost function (optional) [Video]
  - y^ = sigmoid(w^T * x + b) where sigmoid(z) = 1 / (1 + e^-z)
  - y^ = P(y = 1 | x)
  - if y = 1: P(y | x) = y ^
  - if y = 0: P(y | x) = 1 - y^
  - p(y | x) = y^*^*(y * (1 - y^)*^*(1 - y))
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
'''
tags: [
  "machine-learning"
  "deep-learning"
]
isStarred: false
isTrashed: false
