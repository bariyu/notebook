createdAt: "2018-04-01T16:11:54.099Z"
updatedAt: "2018-04-01T21:02:46.341Z"
type: "MARKDOWN_NOTE"
folder: "fc410573c89e5aa6eb30"
title: "Neural Networks and Deep Learning"
content: '''
  # Neural Networks and Deep Learning
  # Week 2
  
  ## Binary Classification [Video]
  - e.g: Image -> 1, 0 (cat or not)
  - Single column feature vector **x**
  - Image size 64x64 => nx/n = 64 * 64 * 3 = 12288
  - x -> y
  - (x, y) x is n dimenional feature vector, y 0 1
  - training examples (x1, y1) .. (xm, ym). m => # of training examples
  - m_train, m_test
  - X => columns as training examples, m columns, nx rows (# of features). X => [nx]**x**[m] matrix. X.shape=(nx, m)
  - Y => m columns of ys. Y => [1]**x**[m] matrix. Y.shape=(1, m)
  
  ## Logistic Regression [Video]
  - Used for binary classification problems
  - given x want y = P(y = 1 | x)
  - x, nx dimentional vector
  - Parameters for logistic regression: **w** nx dimentional vector and **b** real number
  - Output y^ = sigmoid(w^T * x + b).
  - w^T * x + b = z
  - sigmoid(z) = 1 / 1 + e^-z
  - z large => y converges to 1, z small => y converges 0
  
  ## Logistic Regression Cost Function [Video]
  - To train parameters w and b we need a cost funtion
  - want y^(i) ~= y(i)
  - Loss (error) function: L(y^, y) = -(y * logy^ + (1 - y) * log(1 - y^))
  - if y = 1 L(y^, y)  = -logy^, we want logy^: large => y^: large
  - if y = 0 L(y^, y) = -log(1 - y^), we want log(1 - y^): large => y^: small
  - Cost function J(w, b) = 1/m * SUM 1..m : L(y^(i), y(i)) = -1/m * SUM 1..m: [ y(i) * logy^(i) + (1 - y(i)) * log(1 - y^(i)) ]
  - Loss function applied to single training example
  - Cost function is cost of parameters, avg of loss function for all training items
  - Logistic Regression very very small neural network
  
  ## Gradient Descent [Video]
  - algorithm to learn w,b in logistic regression
  - We want to find w,b makes J(w, b) smallest
  - convex surface with 3 dimensions, w, b and J(w, b)
  - w := w - alpha * dJ(w, b) / dw => alpha learning rate * partial derivate
  - b := b - alpha * dJ(w, b) / db => alpha learning rate *  partial derivate 
  
  ## Derivatives [Video]
  - f(a) = 3 * a, f'(a) = 3, df(a)/da = 3, slope
  
  ## More Derivaite Examples [Video]
  - f(a) = a ^ 2, f'(a) = 2 * a
  - f(a) = a ^ z , f'(a) = z * a^(z - 1)
  - f(a) = ln(a), f'(a) = 1 / a
  
  ## Computation graph [Video]
  - J(a, b, c) = 3 * (a + b * c)
  - u = b * c
  - v = a + u
  - J = u * v
  
  ## Derivates with a Computation Graph [Video]
  - chain rule to compute derivates
  
  ## Logistic Regression Gradient Descent [Video]
  - z = w^T * x + b
  - y^ = a = sigmoid(z)
  - L(a, y) = - (y * log(a) + (1 - y) * log(1 - a))
  - derivation, backwards propogation, chain rule
  
  ## Gradient Descent on m Examples [Video]
  - coss function J(w, b) = 1/m * SUM 1..m L(a(i), y)
  - a(i) = y^(i) = sigmoid(z(i)) = sigmoid(w^T * x(i) + b)
  - dJ(w, b)/dw => compute for all w
  ```
  # single step of gradient descent
  # 2 parameters in w
  J, dw1, dw2, db = 0
  for i = 1 to m:
    z(i) = w^T * x(i) + b
    a(i) = sigmoid(z(i))
    Jt = -[ y(i) * loga(i) + (1 - y(i)) * log(1 - a(i)) ]
    dz(i) = a(i) - y(i)
    
    # for loop over all parameters of w actually
    dw1 += x1(i) * dz(i)
    dw2 += x2(i) * dz(i)
    # end for
    
    db += dz(i)
  J, dw1, dw2, db /= m
  
  w1 := w1 - alpha * dw1
  w2 := w2 - alpha * dw2
  b := b - alpha * db
  
  ```
  
'''
tags: [
  "machine-learning"
  "deep-learning"
]
isStarred: false
isTrashed: false
